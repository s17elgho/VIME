{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIobMo2XoH-l"
      },
      "source": [
        "# 0- Introduction\n",
        "This notebook runs a Supervised MLP, a self supervised VIME based on MLP and a semi-supervised VIME on MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f0YbumaroH-o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from utils import mask_generator, pretext_generator\n",
        "from data_loader import load_mnist_data\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "import torchvision\n",
        "import os\n",
        "import pandas as pd\n",
        "from torch.utils.data import (\n",
        "    Dataset,\n",
        "    DataLoader,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3fhGBunoH-p",
        "outputId": "a73b9c11-d154-496d-f633-93078ed26642"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc0a8560bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Nnht137doH-r"
      },
      "outputs": [],
      "source": [
        "from data_sets import TabLabDataset, TabSemiUnlabDataset, TabUnlabDataset, ConcatDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fl8HgqBCoH-r"
      },
      "outputs": [],
      "source": [
        "from vim_self_pytorch import VimPretext\n",
        "from mlp import MLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNZ8M7__oH-s"
      },
      "source": [
        "We use only a subset of 1000  examples as labeled and the rest as unlabeled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-knCsHYoH-s",
        "outputId": "44934a1d-0cb0-4d0d-cd22-3c001e798dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# load the data as numpy arrays first\n",
        "label_data_rate = 0.1  # size of unlabeled = 90% * 60000 = 54000\n",
        "p_m = 0.3 # binomial distribution parameter\n",
        "x_train, y_train, x_unlab, x_test, y_test = load_mnist_data(label_data_rate)\n",
        "label_no = 1000     \n",
        "# Use subset of labeled data\n",
        "x_train = x_train[:label_no, :]\n",
        "y_train = y_train[:label_no, :]  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnHqyNIsoH-t"
      },
      "source": [
        "# 1- Self supervised VIME (working)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58lDLMmioH-t"
      },
      "source": [
        "Let's corrupt the unlabeled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IrCQlN2JoH-u"
      },
      "outputs": [],
      "source": [
        "# corrupt x_unlab\n",
        "m_unlab = mask_generator(p_m, x_unlab)\n",
        "m_label, x_tilde = pretext_generator(m_unlab, x_unlab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5to-V218oH-u",
        "outputId": "7050a23d-1d7b-4727-85a0-d3ba86b21b92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 784), (54000, 784), (54000, 784))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "x_train.shape, x_unlab.shape, x_tilde.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EkVWToBpoH-v"
      },
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Hyperparameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "# Load Data\n",
        "dataset = TabUnlabDataset(x_unlab, x_tilde, m_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_-48UTI6oH-v"
      },
      "outputs": [],
      "source": [
        "#train_set, test_set = torch.utils.data.random_split(dataset, [int(0.7*x_unlab.shape[0]), x_unlab.shape[0] - int(0.7*x_unlab.shape[0])])\n",
        "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
        "#test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True, drop_last = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKARa_TyoH-w"
      },
      "source": [
        "Let's train the self supervised model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JVKtKjAoH-w",
        "outputId": "7e1f716c-c0dc-4b52-ed80-3bd3bc66cabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at epoch 0 is 0.34216887261408807\n",
            "Cost at epoch 1 is 0.27181783113230434\n",
            "Cost at epoch 2 is 0.25780027467677824\n",
            "Cost at epoch 3 is 0.2502126375527393\n",
            "Cost at epoch 4 is 0.24582769893410653\n",
            "Cost at epoch 5 is 0.24341112877863885\n",
            "Cost at epoch 6 is 0.24196037041319804\n",
            "Cost at epoch 7 is 0.24094962075600432\n",
            "Cost at epoch 8 is 0.24020025066836712\n",
            "Cost at epoch 9 is 0.23951620097115034\n"
          ]
        }
      ],
      "source": [
        "model = VimPretext()\n",
        "model.to(device)\n",
        "model.train()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "# Loss and optimizer\n",
        "criterion =torch.nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "    for batch_idx,(x, x_tilde, mask) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        x = x.squeeze().float().to(device=device)\n",
        "        x_tilde = x_tilde.squeeze().float().to(device=device)\n",
        "        mask = mask.squeeze().float().to(device=device)\n",
        "        # forward\n",
        "        mask_lab, feature, encoder = model(x_tilde.float())\n",
        "        loss_rec = loss_function(x.float(), feature)\n",
        "        loss_mask = criterion(mask_lab,mask.float())\n",
        "        loss = 2 * loss_rec + loss_mask\n",
        "        losses.append(loss.item())\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PekcnVd3oH-x"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"self_vim.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dk0IgRrEoH-y",
        "outputId": "1162d61a-ed35-4df7-d89d-7a6720781f2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "VimPretext(\n",
              "  (fc0): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
              "    (1): ReLU()\n",
              "  )\n",
              "  (fc1): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
              "    (1): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpXBMD94oH-y"
      },
      "source": [
        "# 2 - Semi-Supervised VIME (not working correctly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD3V6wfroH-z"
      },
      "source": [
        "We will train on two datasets jointly : the labeled and the unlabeled that's why we will use ConcatDataset class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmscWmE3oH-z"
      },
      "source": [
        "The semi supervised training doesn't work as expected. So I still need to debug this part. I'm suspecting the problem comes from the dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E5_tZ7PLoH-0"
      },
      "outputs": [],
      "source": [
        "# Load Data\n",
        "train_data = TabLabDataset(x_train, y_train)\n",
        "test_data = TabLabDataset(x_test, y_test)\n",
        "unlab_data = TabSemiUnlabDataset(x_unlab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O3Wza6Q3oH-0"
      },
      "outputs": [],
      "source": [
        "class ConcatDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, *datasets):\n",
        "        self.datasets = datasets\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return tuple(d[i %len(d)] for d in self.datasets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return max(len(d) for d in self.datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HtCOoT6uoH-0"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "             ConcatDataset(\n",
        "                 train_data,\n",
        "                 unlab_data\n",
        "             ),\n",
        "             batch_size=128, shuffle=True, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "RjP057YfoH-1"
      },
      "outputs": [],
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "             batch_size=128, shuffle=False, drop_last = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CM0DbFYXoH-1"
      },
      "source": [
        "Inspect the elements of the train_loader : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeRLx8nBoH-2",
        "outputId": "c13a03ef-10e9-44af-c322-027ab3334275"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 784])\n",
            "torch.Size([128, 10])\n",
            "torch.Size([128, 784])\n"
          ]
        }
      ],
      "source": [
        "for i, t in enumerate(train_loader):\n",
        "    print(t[0][0].squeeze().shape)\n",
        "    print(t[0][1].squeeze().shape)\n",
        "    print(t[1].squeeze().shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeF5rbDqoH-2"
      },
      "source": [
        "Let's train the network in a semi supervised fashion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YIHfySvIoH-2"
      },
      "outputs": [],
      "source": [
        "predictor = MLP(28* 28, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG_qipSvoH-3",
        "outputId": "026f344a-3a42-4ae9-8a56-e08bd08f41a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-40-3520f6d96058>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_encoded = model(torch.tensor(x_train).float())[-1]\n",
            "<ipython-input-40-3520f6d96058>:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  m_label, x_tilde = torch.tensor(m_label).float().to(device=device),torch.tensor(x_tilde).float().to(device=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at epoch 0 is 0.015042459866531955\n",
            "Cost at epoch 1 is 0.013333603202181729\n",
            "Cost at epoch 2 is 0.012758135231490515\n",
            "Cost at epoch 3 is 0.012452079456591549\n",
            "Cost at epoch 4 is 0.012222334616452526\n",
            "Cost at epoch 5 is 0.01199833357666847\n",
            "Cost at epoch 6 is 0.0117791113830802\n",
            "Cost at epoch 7 is 0.011609950861639195\n",
            "Cost at epoch 8 is 0.011415664321187697\n",
            "Cost at epoch 9 is 0.011227965155824346\n"
          ]
        }
      ],
      "source": [
        "predictor = predictor.to(device)\n",
        "predictor.train()\n",
        "loss_function = torch.nn.MSELoss()\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(predictor.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(10):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx,(t) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        x_train = t[0][0].squeeze().float().to(device=device)\n",
        "        y_train = t[0][1].squeeze().float().to(device=device)\n",
        "        x_unlab_tr = t[1].squeeze().float().to(device=device)\n",
        "\n",
        "        # encode labeled data\n",
        "        x_encoded = model(torch.tensor(x_train).float())[-1]\n",
        "        y_hat = predictor(x_encoded)\n",
        "        # supervised loss\n",
        "        loss_sup = criterion(y_hat,y_train)\n",
        "\n",
        "\n",
        "        res = torch.empty(size=(5,128, 10))\n",
        "        for i in range(5):\n",
        "            # corrupt x_unlab\n",
        "            m_unlab = mask_generator(p_m, x_unlab_tr)\n",
        "            m_label, x_tilde = pretext_generator(m_unlab, x_unlab_tr.cpu())\n",
        "            m_label, x_tilde = torch.tensor(m_label).float().to(device=device),torch.tensor(x_tilde).float().to(device=device)\n",
        "            x_tilde_encoded = model(x_tilde)[-1]\n",
        "            y_unlab_hat = predictor(x_tilde_encoded)\n",
        "            res[i] = y_unlab_hat\n",
        "        # unsupervised loss\n",
        "        loss_unsup = torch.var(res, dim = 0).mean()\n",
        "        loss = 2*loss_sup + loss_unsup\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QWL3zHioH-3"
      },
      "source": [
        "# 3 - Supervised MLP for benchmark (working)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1czaorLkoH-4"
      },
      "source": [
        "We need to train a supervised MLP on the labeled data : "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "PnD7VGkRoH-4"
      },
      "outputs": [],
      "source": [
        "supervised = MLP(28* 28, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWec0tJloH-4",
        "outputId": "925205ae-214b-473a-dbf8-94a28ac00624"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at epoch 0 is 0.003443149523754248\n",
            "Cost at epoch 1 is 0.0009527591309032343\n",
            "Cost at epoch 2 is 0.0005468992367630157\n",
            "Cost at epoch 3 is 0.0003715672208017818\n",
            "Cost at epoch 4 is 0.0002821891423704151\n",
            "Cost at epoch 5 is 0.00023175906536507812\n",
            "Cost at epoch 6 is 0.00020113462336679961\n",
            "Cost at epoch 7 is 0.000181964211812411\n",
            "Cost at epoch 8 is 0.0001674026492345722\n",
            "Cost at epoch 9 is 0.0001566676162885648\n"
          ]
        }
      ],
      "source": [
        "supervised = supervised.to(device)\n",
        "supervised.train()\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(supervised.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx,(t) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        x_train = t[0][0].squeeze().float().to(device=device)\n",
        "        y_train = t[0][1].squeeze().float().to(device=device)\n",
        "\n",
        "        y_hat = supervised(x_train)\n",
        "        loss = criterion(y_hat,y_train)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCp7_LsboH-5"
      },
      "source": [
        "We will train a supervised MLP using the encoding given by the self supervised training approach :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n46AS3_7oH-5",
        "outputId": "e69e1773-1b29-4a44-c6af-4965d4f98d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-d1a7daeb61dd>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_encoded = model(torch.tensor(x_train).float())[-1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost at epoch 0 is 0.002303593098921502\n",
            "Cost at epoch 1 is 0.000660983803283839\n",
            "Cost at epoch 2 is 0.0003644384759270327\n",
            "Cost at epoch 3 is 0.0002171660401811934\n",
            "Cost at epoch 4 is 0.00014013324672648806\n",
            "Cost at epoch 5 is 0.0001054195455381107\n",
            "Cost at epoch 6 is 8.834262523756444e-05\n",
            "Cost at epoch 7 is 8.066067091676555e-05\n",
            "Cost at epoch 8 is 7.47740829180685e-05\n",
            "Cost at epoch 9 is 7.123622458871684e-05\n"
          ]
        }
      ],
      "source": [
        "self_supervised = MLP(28* 28, 10)\n",
        "self_supervised = self_supervised.to(device)\n",
        "self_supervised.train()\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(self_supervised.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
        "\n",
        "# Train Network\n",
        "for epoch in range(num_epochs):\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx,(t) in enumerate(train_loader):\n",
        "        # Get data to cuda if possible\n",
        "        x_train = t[0][0].squeeze().float().to(device=device)\n",
        "        y_train = t[0][1].squeeze().float().to(device=device)\n",
        "\n",
        "        # encode x_train\n",
        "        \n",
        "        x_encoded = model(torch.tensor(x_train).float())[-1]\n",
        "        y_hat = self_supervised(x_encoded)\n",
        "        loss = criterion(y_hat,y_train)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # gradient descent or adam step\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAEZp-nUoH-6"
      },
      "source": [
        "# Final comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0h_sMI3oH-6"
      },
      "source": [
        "Now Let's compare the performances of the three following models on the same test set : <br>\n",
        "    - Supervised MLP <br>\n",
        "    - Self supervised only MLP with VIME approach <br>\n",
        "    - Semi-supervised MLP with VIME approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zPVXfsZvoH-7"
      },
      "outputs": [],
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy(loader, model):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.squeeze().float().to(device=device)\n",
        "            y = y.squeeze().float().to(device=device)\n",
        "            \n",
        "\n",
        "            scores = model(x.float())\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y.argmax(-1)).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct / num_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Kvzfo2vXoH-7"
      },
      "outputs": [],
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy_self(loader, model, encoder):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.squeeze().float().to(device=device)\n",
        "            y = y.squeeze().float().to(device=device)\n",
        "            x_encoded = encoder(torch.tensor(x).float())[-1]\n",
        "            scores = model(x_encoded.float())\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y.argmax(-1)).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct / num_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "L1f0uWSioH-7"
      },
      "outputs": [],
      "source": [
        "# Check accuracy on training & test to see how good our model\n",
        "def check_accuracy_semi(loader, model, encoder):\n",
        "    num_correct = 0\n",
        "    num_samples = 0\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x = x.squeeze().float().to(device=device)\n",
        "            y = y.squeeze().float().to(device=device)\n",
        "            x_encoded = encoder(torch.tensor(x).float())[-1]\n",
        "            scores = model(x_encoded.float())\n",
        "            _, predictions = scores.max(1)\n",
        "            num_correct += (predictions == y.argmax(-1)).sum()\n",
        "            num_samples += predictions.size(0)\n",
        "\n",
        "    model.train()\n",
        "    return num_correct / num_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ciiVHqUyoH-8",
        "outputId": "3f39beeb-ec3d-491f-d1ed-01e683036d03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-721661cf10f8>:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x_encoded = encoder(torch.tensor(x).float())[-1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test set for the semi supervised approach: 89.43\n",
            "Accuracy on test set for the supervised only approach: 88.81\n",
            "Accuracy on test set for the self supervised approach: 90.97\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy on test set for the semi supervised approach: {check_accuracy_semi(test_loader, predictor,model)*100:.2f}\")\n",
        "print(f\"Accuracy on test set for the supervised only approach: {check_accuracy(test_loader, supervised)*100:.2f}\")\n",
        "print(f\"Accuracy on test set for the self supervised approach: {check_accuracy_self(test_loader, self_supervised, model)*100:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "vime",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d35eafea3a53c02e8e93e7a94ca67b47366c400246dda0c0deeefb7cf277a87d"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}