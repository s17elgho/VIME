{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0- Introduction\n",
    "This notebook runs a Supervised MLP, a self supervised VIME based on MLP and a semi-supervised VIME on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import mask_generator, pretext_generator\n",
    "from data_loader import load_mnist_data\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x212eef56050>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_sets import TabLabDataset, TabSemiUnlabDataset, TabUnlabDataset, ConcatDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vim_self_pytorch import VimPretext\n",
    "from mlp import MLP\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only a subset of 1000  examples as labeled and the rest as unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data as numpy arrays first\n",
    "label_data_rate = 0.1  # size of unlabeled = 90% * 60000 = 54000\n",
    "p_m = 0.3 # binomial distribution parameter\n",
    "x_train, y_train, x_unlab, x_test, y_test = load_mnist_data(label_data_rate)\n",
    "label_no = 1000     \n",
    "# Use subset of labeled data\n",
    "x_train = x_train[:label_no, :]\n",
    "y_train = y_train[:label_no, :]  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Self supervised VIME (working)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's corrupt the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrupt x_unlab\n",
    "m_unlab = mask_generator(p_m, x_unlab)\n",
    "m_label, x_tilde = pretext_generator(m_unlab, x_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 784), (54000, 784), (54000, 784))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_unlab.shape, x_tilde.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Hyperparameters\n",
    "learning_rate = 3e-4\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "# Load Data\n",
    "dataset = TabUnlabDataset(x_unlab, x_tilde, m_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_set, test_set = torch.utils.data.random_split(dataset, [int(0.7*x_unlab.shape[0]), x_unlab.shape[0] - int(0.7*x_unlab.shape[0])])\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, drop_last = True)\n",
    "#test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True, drop_last = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the self supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 0.3419393597617568\n",
      "Cost at epoch 1 is 0.2715295735843957\n",
      "Cost at epoch 2 is 0.2576055600354337\n",
      "Cost at epoch 3 is 0.24999438661577583\n",
      "Cost at epoch 4 is 0.24558537253433055\n",
      "Cost at epoch 5 is 0.24316236560814738\n",
      "Cost at epoch 6 is 0.2417228785022704\n",
      "Cost at epoch 7 is 0.240718988369876\n",
      "Cost at epoch 8 is 0.23993116921597024\n",
      "Cost at epoch 9 is 0.2392337374619237\n"
     ]
    }
   ],
   "source": [
    "model = VimPretext()\n",
    "model.to(device)\n",
    "model.train()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "# Loss and optimizer\n",
    "criterion =torch.nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_idx,(x, x_tilde, mask) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        x = x.squeeze().float().to(device=device)\n",
    "        x_tilde = x_tilde.squeeze().float().to(device=device)\n",
    "        mask = mask.squeeze().float().to(device=device)\n",
    "        # forward\n",
    "        mask_lab, feature, encoder = model(x_tilde.float())\n",
    "        loss_rec = loss_function(x.float(), feature)\n",
    "        loss_mask = criterion(mask_lab,mask.float())\n",
    "        loss = 2 * loss_rec + loss_mask\n",
    "        losses.append(loss.item())\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"self_vim.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VimPretext(\n",
       "  (fc0): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (fc1): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=784, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Semi-Supervised VIME (not working correctly)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train on two datasets jointly : the labeled and the unlabeled that's why we will use ConcatDataset class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semi supervised training doesn't work as expected. So I still need to debug this part. I'm suspecting the problem comes from the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_data = TabLabDataset(x_train, y_train)\n",
    "test_data = TabLabDataset(x_test, y_test)\n",
    "unlab_data = TabSemiUnlabDataset(x_unlab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(d[i %len(d)] for d in self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(d) for d in self.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "             ConcatDataset(\n",
    "                 train_data,\n",
    "                 unlab_data\n",
    "             ),\n",
    "             batch_size=128, shuffle=True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "             batch_size=128, shuffle=False, drop_last = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the elements of the train_loader : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n",
      "torch.Size([128, 10])\n",
      "torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "for i, t in enumerate(train_loader):\n",
    "    print(t[0][0].squeeze().shape)\n",
    "    print(t[0][1].squeeze().shape)\n",
    "    print(t[1].squeeze().shape)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network in a semi supervised fashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = MLP(28* 28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma El Ghourbal\\AppData\\Local\\Temp\\ipykernel_456\\632656810.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_encoded = model(torch.tensor(x_train).float())[-1]\n",
      "C:\\Users\\Salma El Ghourbal\\AppData\\Local\\Temp\\ipykernel_456\\632656810.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_tilde_encoded = model(torch.tensor(x_tilde).float())[-1]\n",
      "C:\\Users\\Salma El Ghourbal\\AppData\\Local\\Temp\\ipykernel_456\\632656810.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_un_encoded = model(torch.tensor(x_unlab_tr).float())[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 0.009099787882044734\n",
      "Cost at epoch 1 is 0.009009868660919598\n",
      "Cost at epoch 2 is 0.008987508652258372\n",
      "Cost at epoch 3 is 0.008969726242266614\n",
      "Cost at epoch 4 is 0.008936467197792263\n",
      "Cost at epoch 5 is 0.008905681883230487\n",
      "Cost at epoch 6 is 0.008882651286920855\n",
      "Cost at epoch 7 is 0.008825121690782402\n"
     ]
    }
   ],
   "source": [
    "predictor = predictor.to(device)\n",
    "predictor.train()\n",
    "loss_function = torch.nn.MSELoss()\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(predictor.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx,(t) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        x_train = t[0][0].squeeze().float().to(device=device)\n",
    "        y_train = t[0][1].squeeze().float().to(device=device)\n",
    "        x_unlab_tr = t[1].squeeze().float().to(device=device)\n",
    "\n",
    "        # encode labeled data\n",
    "        x_encoded = model(torch.tensor(x_train).float())[-1]\n",
    "        y_hat = predictor(x_encoded)\n",
    "        # supervised loss\n",
    "        loss_sup = criterion(y_hat,y_train)\n",
    "        loss_unsup = 0\n",
    "        for i in range(5):\n",
    "            # corrupt x_unlab\n",
    "            m_unlab = mask_generator(p_m, x_unlab_tr)\n",
    "            m_label, x_tilde = pretext_generator(m_unlab, x_unlab_tr)\n",
    "            x_tilde_encoded = model(torch.tensor(x_tilde).float())[-1]\n",
    "            y_unlab_hat = predictor(x_tilde_encoded)\n",
    "\n",
    "\n",
    "            x_un_encoded = model(torch.tensor(x_unlab_tr).float())[-1]\n",
    "            y_unlab = predictor(x_un_encoded)\n",
    "            loss_unsup += loss_function(y_unlab,y_unlab_hat)\n",
    "        # unsupervised loss\n",
    "        loss_unsup_mean = loss_unsup/5\n",
    "        loss = loss_sup + 2* loss_unsup_mean\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Supervised MLP for benchmark (working)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to train a supervised MLP on the labeled data : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised = MLP(28* 28, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 0.008954781772834914\n",
      "Cost at epoch 1 is 0.008669431321322918\n",
      "Cost at epoch 2 is 0.008353438095322676\n",
      "Cost at epoch 3 is 0.008019167863364731\n",
      "Cost at epoch 4 is 0.007654338942042419\n",
      "Cost at epoch 5 is 0.007310390538935151\n",
      "Cost at epoch 6 is 0.006951497601611274\n",
      "Cost at epoch 7 is 0.006588164783482041\n",
      "Cost at epoch 8 is 0.006263115177197116\n",
      "Cost at epoch 9 is 0.005957594806594508\n"
     ]
    }
   ],
   "source": [
    "supervised = supervised.to(device)\n",
    "supervised.train()\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(supervised.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx,(t) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        x_train = t[0][0].squeeze().float().to(device=device)\n",
    "        y_train = t[0][1].squeeze().float().to(device=device)\n",
    "\n",
    "        y_hat = supervised(x_train)\n",
    "        loss = criterion(y_hat,y_train)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a supervised MLP using the encoding given by the self supervised training approach :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma El Ghourbal\\AppData\\Local\\Temp\\ipykernel_456\\3389353923.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_encoded = model(torch.tensor(x_train).float())[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 0.008675807820899146\n",
      "Cost at epoch 1 is 0.007869995731328214\n",
      "Cost at epoch 2 is 0.0071040018062506404\n",
      "Cost at epoch 3 is 0.006337571223931653\n",
      "Cost at epoch 4 is 0.005657581612467766\n",
      "Cost at epoch 5 is 0.0050347282418182916\n",
      "Cost at epoch 6 is 0.004467035510710308\n",
      "Cost at epoch 7 is 0.004025617381557822\n",
      "Cost at epoch 8 is 0.003681020312277334\n",
      "Cost at epoch 9 is 0.0033261486927845646\n"
     ]
    }
   ],
   "source": [
    "self_supervised = MLP(28* 28, 10)\n",
    "self_supervised = self_supervised.to(device)\n",
    "self_supervised.train()\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(self_supervised.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx,(t) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        x_train = t[0][0].squeeze().float().to(device=device)\n",
    "        y_train = t[0][1].squeeze().float().to(device=device)\n",
    "\n",
    "        # encode x_train\n",
    "        \n",
    "        x_encoded = model(torch.tensor(x_train).float())[-1]\n",
    "        y_hat = self_supervised(x_encoded)\n",
    "        loss = criterion(y_hat,y_train)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's compare the performances of the three following models on the same test set : <br>\n",
    "    - Supervised MLP <br>\n",
    "    - Self supervised only MLP with VIME approach <br>\n",
    "    - Semi-supervised MLP with VIME approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.squeeze().float().to(device=device)\n",
    "            y = y.squeeze().float().to(device=device)\n",
    "            \n",
    "\n",
    "            scores = model(x.float())\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y.argmax(-1)).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy_self(loader, model, encoder):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.squeeze().float().to(device=device)\n",
    "            y = y.squeeze().float().to(device=device)\n",
    "            x_encoded = encoder(torch.tensor(x).float())[-1]\n",
    "            scores = model(x_encoded.float())\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y.argmax(-1)).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy_semi(loader, model, encoder):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.squeeze().float().to(device=device)\n",
    "            y = y.squeeze().float().to(device=device)\n",
    "            x_encoded = encoder(torch.tensor(x).float())[-1]\n",
    "            scores = model(x_encoded.float())\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y.argmax(-1)).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Salma El Ghourbal\\AppData\\Local\\Temp\\ipykernel_456\\1829281224.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_encoded = encoder(torch.tensor(x).float())[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set for the semi supervised approach: 59.41\n",
      "Accuracy on test set for the supervised only approach: 76.76\n",
      "Accuracy on test set for the self supervised approach: 83.36\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on test set for the semi supervised approach: {check_accuracy_semi(test_loader, predictor,model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set for the supervised only approach: {check_accuracy(test_loader, supervised)*100:.2f}\")\n",
    "print(f\"Accuracy on test set for the self supervised approach: {check_accuracy_self(test_loader, self_supervised, model)*100:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d35eafea3a53c02e8e93e7a94ca67b47366c400246dda0c0deeefb7cf277a87d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
